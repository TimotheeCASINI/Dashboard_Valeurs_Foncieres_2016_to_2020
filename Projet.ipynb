{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization - Projet\n",
    "## Projet: Demandes de valeurs foncières\n",
    "\n",
    "- Timothée CASINI\n",
    "- ST2DVZ - DS6\n",
    "- 15/10/2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description du projet\n",
    "\n",
    "L'idée du projet est assez simple : Appliquer le processus d'exploration visuelle de données que nous avons vu pendant les labs au jeu de données \"**Demandes de foncières valeurs**\".\n",
    "\n",
    "Le jeu de données est disponible via ce lien : <https://drive.google.com/drive/folders/1R_9A9yPOzRQzMCyTDBEJms0u1ZCN7MbY>\n",
    "\n",
    "Le dossier contient 5 fichiers CSV et 2 fichiers pdf : \n",
    "* 1 pour la FAQ\n",
    "* 1 pour la description des colonnes\n",
    "\n",
    "Le jeu de données est accessible au public sur le site ***data.gouv.fr*** : <https://www.data.gouv.fr/en/datasets/demandes-de-valeurs-foncieres/>\n",
    "\n",
    "*Vous pouvez trouver plus de contexte dans ce lien !*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rappel\n",
    "\n",
    "Le processus d'exploration visuelle des données peut être divisé en 4 étapes :\n",
    "\n",
    "1. **Chargement des données** : Import des bibliothèques et structure de données \n",
    "\n",
    "2. **Explorer et traiter** : Nettoyage, prétraitement, transformation et enrichissement des données\n",
    "\n",
    "3. **Visualisation des données** :Représentation visuelle et analyse\n",
    "\n",
    "4. **Extraction d'informations** : soutenir les résultats analytiques et la prise de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectifs et exigences du projet :\n",
    "\n",
    "L'objectif de ce projet est de créer une application simplifiée et de la publier avec un partage simplifié. \n",
    "\n",
    "Comme le jeu de données est très riche, vous avez beaucoup de possibilités et vous êtes libre d'imaginer et de créer les tableaux de bord que vous souhaitez avec les visualisations qui vous intéressent le plus. \n",
    "\n",
    "Soyez curieux et essayez de vous mettre à l'aise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cependant, vous devez suivre quelques consignes et respecter quelques exigences minimales :\n",
    "\n",
    "1. Vous disposez de 5 fichiers csv et devez utiliser au MINIMUM : `full_2020.csv`\n",
    "\n",
    "2. Lire la FAQ et la notice descriptive avant commencer à coder\n",
    "\n",
    "3. Utiliser l'un des deux packages python EDA automatisés dans un notebook distinct afin d'explorer rapidement les fichiers (***Facultatif***)\n",
    "\n",
    "4. Votre application streamlit doit respecter les exigences suivantes :\n",
    "\n",
    "- Organiser votre code en fonctions modulaires\n",
    "       \n",
    "- 2 plot interne streamlit : `st.line` ou `st.bar_chart` ET `st.map`\n",
    "       \n",
    "- 4 graphiques externes différents intégrés à votre application à partir de bibliothèques externes\n",
    "       \n",
    "- 2 checkboxs qui interagissent avec votre jeu de données\n",
    "       \n",
    "- Un slider qui intéragit avec un ou plusieurs plots\n",
    "       \n",
    "- Au minimum un cache pour le chargement et le pré-traitement des données avec `st.cache`\n",
    "       \n",
    "- Un décorateur qui consigne dans un fichier l'intervalle de temps d'exécution en secondes et l'horodatage de l'appel de la fonction appelée via le décorateur\n",
    "       \n",
    "- Facultatif : essayez d'organiser vos appels de fonctions en une fonction principale afin d'avoir un workflow clair de votre application\n",
    "\n",
    "- Facultatif : Intégrez tous les fichiers ensemble afin d'avoir une perspective sur plusieurs années\n",
    "\n",
    "- Facultatif : Imaginez des formulaires - textes, dates ou chiffres - pour récupérer les saisies de vos utilisateurs et leur restituer des analyses et des visualisations correspondant à leurs critères de recherche et de choix\n",
    "\n",
    "\n",
    "Conseils : Définir une fonction main() et à l'appeler à la fin avec \n",
    "       if __name__ == \"__main__\":\n",
    "              main()\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Profilage des pandas : <https://pandas-profiling.github.io>\n",
    "##### Sweetviz : <https://github.com/fbdesignpro/sweetviz>\n",
    "\n",
    "ps : Assurez-vous de rétrograder votre python à 3.8 pour les package EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réalisation du projet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation du projet\n",
    "\n",
    "On commnence par créer un environnement virtuel depuis un terminal dans le dossier Projet afin de pouvoir avoir toute les dépendances dans un même dossier :\n",
    "\n",
    "* Installalation préalable d'un environnement virtuel dédié au projet `python3 -m venv ./venvdtv`\n",
    "\n",
    "* Activation de l'environnement `source ./venvdtv/bin/activate`\n",
    "\n",
    "* Installation de l'ipyKernel dans l'environnement virtuel `pip install -U ipykernel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de l'environnement virtuel:\n",
    "#!python3 -m venv ./venvdtv\n",
    "\n",
    "# Activation de l'environnement virtuel:\n",
    "#!source venvdtv/bin/activate\n",
    "\n",
    "# Installation de l'ipyKernel dans l'environnement virtuel:\n",
    "#!pip install -U ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois cela fait, on utilise le ipykernel du `vendtv` pour l'installation des packages et l'éxécution du code.\n",
    "\n",
    "<br>\n",
    "\n",
    "Maintenant on installe toute le bibliothèques nécessaires et on créer le dossier source :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation de streamlit:\n",
    "#!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des bibliothèques pour l'analyse de données:\n",
    "#!pip install numpy\n",
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des bibliothèques de visualisations:\n",
    "#!pip install matplotlib\n",
    "#!pip install seaborn\n",
    "#!pip install altair\n",
    "#!pip install pydeck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le dossier 'src' a été créé avec succès !\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# On créer le dossier source contenant le code:\n",
    "directory = \"src\"\n",
    "parent_dir = os.path.dirname(os.path.abspath('Projet.ipynb'))\n",
    "  \n",
    "path = os.path.join(parent_dir, directory)\n",
    "\n",
    "try:\n",
    "    os.makedirs(path, exist_ok = True)\n",
    "    print(\"Le dossier '%s' a été créé avec succès !\" % directory)\n",
    "except OSError as error:\n",
    "    print(\"Le dossier '%s' n'a pas pu être effectuée\" % directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données\n",
    "\n",
    "* Import des bibliothèques\n",
    "* Initialisation de l'application\n",
    "* Import des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/projet_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/projet_app.py\n",
    "### Timothée CASINI - DS§ - Projet Data Visualization ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/projet_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/projet_app.py\n",
    "\n",
    "# Bibliothèques:\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import datetime as dt\n",
    "import streamlit as st\n",
    "import pydeck as pdk\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisation de l'application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/projet_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/projet_app.py\n",
    "\n",
    "writepath = './src/running_time.txt'\n",
    "\n",
    "# Fonction d'hortodatage et de file d'exécution:\n",
    "def init_file(writepath):\n",
    "    if os.path.exists(writepath):\n",
    "        mode ='a'\n",
    "    else:\n",
    "        mode ='w'\n",
    "    with open(writepath, mode) as file:\n",
    "        dt_string = '##########  Exécution du '\n",
    "        dt_string += d.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        dt_string += '  ##########\\n'\n",
    "\n",
    "        file.write(dt_string)\n",
    "\n",
    "init_file(writepath)\n",
    "\n",
    "# Décorateur avec temps d'execution:\n",
    "def log(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        val = func(*args, **kwargs)\n",
    "        with open(writepath, \"a+\") as f:\n",
    "            f.write( \"Fonction = \" + func.__name__ + \" | Temps d'éxécution = \" + str(time.time() - start_time)+ \"\\n\" )\n",
    "        return val\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "# Fonction de d'initiation de l'app:\n",
    "@log\n",
    "def init_app():\n",
    "    st.title('Dashboard Valeurs Foncières')\n",
    "\n",
    "    st.header('Projet Data Visualization - Timothée CASINI - DS6')\n",
    "\n",
    "    st.subheader('Données portant sur les valeurs foncières déclarées de 2016 à 2020')\n",
    "\n",
    "    st.markdown(\"<p style='text-align: justify; color: grey; position:left;'>Ce projet à pour objectif d'appliquer le processus d'exploration visuelle de données que nous avons vu pendant les labs au jeu de données 'Demandes de foncières valeurs'. Nous allons visualiser à travers différents diagrammes les données mais également leurs appliquées différents traitements et les explorer sous plusieurs formes afin d'en faire ressortir des annalyses pertinantes. \",unsafe_allow_html=True)\n",
    "\n",
    "init_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/projet_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/projet_app.py\n",
    "\n",
    "# Chargement des donées:\n",
    "file_2016 = './data/full_2016.csv'\n",
    "file_2017 = './data/full_2017.csv'\n",
    "file_2018 = './data/full_2018.csv'\n",
    "file_2019 = './data/full_2019.csv'\n",
    "file_2020 = './data/full_2020.csv'\n",
    "\n",
    "files = [file_2016, file_2017, file_2018, file_2019, file_2020]\n",
    "@st.cache(suppress_st_warning=True, allow_output_mutation=True)\n",
    "@log\n",
    "def init_df(files):\n",
    "\n",
    "     df_2016 = pd.read_csv(\n",
    "          files[0],\n",
    "          header=0,low_memory=False)\n",
    "\n",
    "     df_2017 = pd.read_csv(\n",
    "          files[1],\n",
    "          header=0,low_memory=False)\n",
    "\n",
    "     df_2018 = pd.read_csv(\n",
    "          files[2],\n",
    "          header=0,low_memory=False)\n",
    "\n",
    "     df_2019 = pd.read_csv(\n",
    "          files[3],\n",
    "          header=0,low_memory=False)\n",
    "\n",
    "     df_2020 = pd.read_csv(\n",
    "          files[4],\n",
    "          header=0,low_memory=False)\n",
    "\n",
    "     return df_2016, df_2017, df_2018, df_2019, df_2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traitements des données\n",
    "\n",
    "* Nettoyage des données\n",
    "* Transformation des données \n",
    "* Fonctions de traitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nettoyage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/projet_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/projet_app.py\n",
    "\n",
    "# Taux de valeurs manquantes:\n",
    "@st.cache(suppress_st_warning=True, allow_output_mutation=True)\n",
    "@log\n",
    "def missing_rate(df):\n",
    "    miss_data = df.isna()\n",
    "    miss_data = miss_data.sum()\n",
    "    miss_data = miss_data/len(df)\n",
    "    miss_data_bool = miss_data > 0.5\n",
    "\n",
    "    return miss_data[miss_data_bool == True]\n",
    "\n",
    "# Suppression des données manquantes :\n",
    "@st.cache(suppress_st_warning=True, allow_output_mutation=True)\n",
    "@log\n",
    "def cleaning(df, indexes):\n",
    "    df = df.drop(labels=indexes, axis=1)\n",
    "    \n",
    "    df['valeur_fonciere'] = df['valeur_fonciere'].fillna(value=df['valeur_fonciere'].mean()) # on affecte la moyenne aux éléments vides\n",
    "    df['surface_terrain'] = df['surface_terrain'].fillna(value=df['surface_terrain'].mean())\n",
    "    df['nombre_pieces_principales'] = df['nombre_pieces_principales'].fillna(value=df['nombre_pieces_principales'].mean())\n",
    "    \n",
    "    for i in df.columns:\n",
    "        if (df[i].isnull().values.any() == True):\n",
    "            df.dropna(subset = [i,], inplace=True) # supression ligne si vide\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/projet_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/projet_app.py\n",
    "\n",
    "# Transformation des types:\n",
    "@st.cache(suppress_st_warning=True, allow_output_mutation=True)\n",
    "@log\n",
    "def pre_processing(df):\n",
    "    for y in df.columns:\n",
    "        if(df[y].dtype == object):\n",
    "            df[y] = df[y].convert_dtypes()\n",
    "    df['date_mutation'] = pd.to_datetime(df['date_mutation'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonctions de traitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/projet_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/projet_app.py\n",
    "\n",
    "# Mois:\n",
    "def get_mth(dt): \n",
    "    return dt.month\n",
    "\n",
    "# Jour du mois:\n",
    "def get_dom(dt): \n",
    "    return dt.day\n",
    "    \n",
    "# Nombre de lignes:\n",
    "def get_rows(rows):\n",
    "    return len(rows)\n",
    "\n",
    "# Prix par mètre carré:\n",
    "def get_price_by_square (df):\n",
    "    return (df.valeur_fonciere / df.surface_terrain)\n",
    "\n",
    "# Région:\n",
    "def get_regions (code_departement):\n",
    "    auvergne = ['01','03','07','15','26','38','42','43','63','69','73','74']\n",
    "    bourgogne = ['21','25','39','58','70','71','89','90']\n",
    "    bretagne = ['22','29','35','56']\n",
    "    val_loire = ['18','28','36','37','41','45']\n",
    "    corse = ['20']\n",
    "    ile_de_france = ['75','77','78','91','92','93','94','95']\n",
    "    est = ['08','10','51','52','54','55','57','67','68','88']\n",
    "    haut_france = ['02','59','60','62','80']\n",
    "    normandie = ['14','27','50','61','76']\n",
    "    aquitaine = ['16','17','19','23','24','33','40','47','64','79','86','87']\n",
    "    occitanie = ['09','11','12','30','31','32','34','46','48','65','66','81','82']\n",
    "    pays_loire = ['44','49','53','72','85']\n",
    "    provence = ['04','05','06','13','83','84']\n",
    "\n",
    "    region =''\n",
    "\n",
    "    if (code_departement in auvergne):\n",
    "        region = \"AUVERGNE-RHÔNE-ALPES\"\n",
    "\n",
    "    elif (code_departement in bourgogne):\n",
    "        region = \"BOURGOGNE-FRANCHE-COMTÉ\"\n",
    "    \n",
    "    elif (code_departement in bretagne):\n",
    "        region = \"BRETAGNE\"\n",
    "    \n",
    "    elif (code_departement in val_loire):\n",
    "        region = \"CENTRE-VAL-DE-LOIRE\"\n",
    "    \n",
    "    elif (code_departement in corse):\n",
    "        region = \"CORSE\"\n",
    "\n",
    "    elif (code_departement in ile_de_france):\n",
    "        region = \"ÎLE-DE-FRANCE\"\n",
    "\n",
    "    elif (code_departement in est):\n",
    "        region = \"GRAND-EST\"\n",
    "\n",
    "    elif (code_departement in haut_france):\n",
    "        region = \"HAUT-DE-FRANCE\"\n",
    "\n",
    "    elif (code_departement in normandie):\n",
    "        region = \"NORMANDIE\"\n",
    "\n",
    "    elif (code_departement in aquitaine):\n",
    "        region = \"NOUVELLE-AQUITAINE\"\n",
    "\n",
    "    elif (code_departement in occitanie):\n",
    "        region = \"OCCITANIE\"\n",
    "\n",
    "    elif (code_departement in pays_loire):\n",
    "        region = \"PAYS-DE-LA-LOIRE\"\n",
    "\n",
    "    elif (code_departement in provence):\n",
    "        region = \"PROVENCE-ALPES-CÔTE D'AZUR\"\n",
    "    \n",
    "    else:\n",
    "        region = \"CORSE\"\n",
    "\n",
    "    return region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/projet_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/projet_app.py\n",
    "\n",
    "@log\n",
    "@st.cache(suppress_st_warning=True)\n",
    "def process_square(df):\n",
    "\n",
    "    # On regroupe les données selon le prix par mètre carré moyen:\n",
    "    group_by_square = df.groupby(['code_departement','region']).agg({'surface_terrain':'mean','valeur_fonciere':'mean'})\n",
    "    \n",
    "    return group_by_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/projet_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/projet_app.py\n",
    "\n",
    "@log\n",
    "@st.cache(suppress_st_warning=True)\n",
    "def process_commune(df):\n",
    "    \n",
    "    # On regroupe les données par commune:\n",
    "    group_by_commune = df.groupby('nom_commune').size()\n",
    "    group_by_commune = group_by_commune.sort_values()\n",
    "\n",
    "    return group_by_commune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/projet_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/projet_app.py\n",
    "\n",
    "@log\n",
    "@st.cache(suppress_st_warning=True)\n",
    "def process_commune_square(df):\n",
    "\n",
    "    # On regroupe les données selon le prix par mètre carré moyen:\n",
    "    group_by_commune_square = df.groupby('nom_commune').agg({'price_by_square':'mean'})\n",
    "    group_by_commune_square = group_by_commune_square.sort_values(by='price_by_square')\n",
    "    \n",
    "    return group_by_commune_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/projet_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/projet_app.py\n",
    "\n",
    "@log\n",
    "@st.cache(suppress_st_warning=True)\n",
    "def process_commune_departement_square(df):\n",
    "\n",
    "    # On regroupe les données selon le prix par mètre carré moyen:\n",
    "    group_by_commune_departement_square = df.groupby(['code_departement','nom_commune']).agg({'price_by_square':'mean'})\n",
    "\n",
    "    \n",
    "    return group_by_commune_departement_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/projet_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/projet_app.py\n",
    "\n",
    "@log\n",
    "@st.cache(suppress_st_warning=True)\n",
    "def process_departement(df):\n",
    "\n",
    "    # On regroupe les données par département:\n",
    "    group_by_departement = df.groupby('code_departement').size()\n",
    "    group_by_departement = group_by_departement.sort_values()\n",
    "    \n",
    "    return group_by_departement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/projet_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/projet_app.py\n",
    "\n",
    "@log\n",
    "@st.cache(suppress_st_warning=True)\n",
    "def process_departement_square(df):\n",
    "\n",
    "    # On regroupe les données selon le prix par mètre carré moyen:\n",
    "    group_by_departement_square = df.groupby('code_departement').agg({'price_by_square':'mean'})\n",
    "    group_by_departement_square = group_by_departement_square.sort_values(by='price_by_square')\n",
    "    \n",
    "    return group_by_departement_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/projet_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/projet_app.py\n",
    "\n",
    "@log\n",
    "@st.cache(suppress_st_warning=True)\n",
    "def process_month(df):\n",
    "    group_by_dom = df.groupby('dom').size()\n",
    "    \n",
    "    return group_by_dom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/projet_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/projet_app.py\n",
    "\n",
    "@log\n",
    "@st.cache(suppress_st_warning=True)\n",
    "def process_region(df):\n",
    "    group_by_depatement_region = df.groupby(['region','code_departement']).size().unstack(level=0)\n",
    "    group_by_depatement_region = group_by_depatement_region.fillna(0)\n",
    "    return group_by_depatement_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/projet_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/projet_app.py\n",
    "\n",
    "@log\n",
    "@st.cache(suppress_st_warning=True)\n",
    "def process_local(df):\n",
    "    group_by_local = df.groupby(['code_departement','type_local']).size().unstack(level=1)\n",
    "    group_by_local = group_by_local.fillna(0)\n",
    "    return group_by_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/projet_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/projet_app.py\n",
    "\n",
    "@log\n",
    "@st.cache(suppress_st_warning=True)\n",
    "def process_local_region(df):\n",
    "    group_by_local_region = df.groupby(['region','type_local']).size().unstack(level=1)\n",
    "    group_by_local_region = group_by_local_region.fillna(0)\n",
    "    return group_by_local_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/projet_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/projet_app.py\n",
    "\n",
    "# Traitement des données:\n",
    "@log\n",
    "@st.cache(suppress_st_warning=True)\n",
    "def processing(df):\n",
    "\n",
    "    df['mth']  = df['date_mutation'].map(get_mth)\n",
    "    df['dom']  = df['date_mutation'].map(get_dom)\n",
    "    df['price_by_square'] = get_price_by_square(df)\n",
    "    df['region']  = df['code_departement'].map(get_regions)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def croissance_rate(option_data, df):\n",
    "    if (option_data == 'Valeurs Foncières 2017'):\n",
    "        \n",
    "        df1 = init_df(file_2016)\n",
    "        miss1 = missing_rate(df1)\n",
    "\n",
    "        df1 = cleaning(df1, miss1.index)\n",
    "        df1 = pre_processing(df1)\n",
    "        df1 = processing(df1) \n",
    "        \n",
    "        years_n = df['valeur_fonciere'].sum()\n",
    "        years_n_1 = df1['valeur_fonciere'].sum()\n",
    "        croissance = ((years_n - years_n_1)/years_n_1)*100\n",
    "\n",
    "        return croissance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation et exploration des données\n",
    "\n",
    "* Initialisation des données\n",
    "* Initialisation de la sidebar\n",
    "* Visualisation des graphiques\n",
    "* Visualisation des analyses et des traitements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/projet_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/projet_app.py\n",
    "\n",
    "option_data = st.sidebar.selectbox(\n",
    "            \"Quels données voulez vous choisir ? \", ['Valeurs Foncières 2016','Valeurs Foncières 2017', 'Valeurs Foncières 2018', 'Valeurs Foncières 2019', 'Valeurs Foncières 2020'])\n",
    "    \n",
    "# Choix du dataframe:\n",
    "@log\n",
    "def init_data():\n",
    "    df_2016, df_2017, df_2018, df_2019, df_2020 = init_df(files)\n",
    "    \n",
    "    if (option_data == 'Valeurs Foncières 2016'):\n",
    "            df = df_2016\n",
    "            st.markdown('##### Données des valeurs foncières déclarées en 2016')\n",
    "            st.write(\"On observe de nombreuse données manquantes, mal codées ou nécessitant un pré-traitement. Nous allons d'abords nettoyer les données en supprimans les colonnes où le pourcentage de données manquantes est trop importantes. Ensuite, nous supprimerons les lignes où des données manquent encore à l'appel. On intégrera de nouvelles données et on transformera les colonnes dont le type n'est pas correct. On inserera de nouvelles colonne et on remplira celle dont les valeurs manquantes peuvent être déduites.\")\n",
    "\n",
    "            st.caption('Dataframe initial')\n",
    "            st.dataframe(df.head())\n",
    "\n",
    "    elif (option_data == 'Valeurs Foncières 2017'):\n",
    "            df = df_2017\n",
    "            st.markdown('##### Données des valeurs foncières déclarées en 2017')\n",
    "            st.write(\"On observe de nombreuse données manquantes, mal codées ou nécessitant un pré-traitement. Nous allons d'abords nettoyer les données en supprimans les colonnes où le pourcentage de données manquantes est trop importantes. Ensuite, nous supprimerons les lignes où des données manquent encore à l'appel. On intégrera de nouvelles données et on transformera les colonnes dont le type n'est pas correct. On inserera de nouvelles colonne et on remplira celle dont les valeurs manquantes peuvent être déduites.\")\n",
    "\n",
    "            st.caption('Dataframe initial')\n",
    "            st.dataframe(df.head())\n",
    "\n",
    "    elif (option_data == 'Valeurs Foncières 2018'):\n",
    "            df = df_2018\n",
    "            st.markdown('##### Données des valeurs foncières déclarées en 2018')\n",
    "            st.write(\"On observe de nombreuse données manquantes, mal codées ou nécessitant un pré-traitement. Nous allons d'abords nettoyer les données en supprimans les colonnes où le pourcentage de données manquantes est trop importantes. Ensuite, nous supprimerons les lignes où des données manquent encore à l'appel. On intégrera de nouvelles données et on transformera les colonnes dont le type n'est pas correct. On inserera de nouvelles colonne et on remplira celle dont les valeurs manquantes peuvent être déduites.\")\n",
    "            st.caption('Dataframe initial')\n",
    "            st.dataframe(df.head())\n",
    "\n",
    "    elif (option_data == 'Valeurs Foncières 2019'):\n",
    "            df = df_2019\n",
    "            st.markdown('##### Données des valeurs foncières déclarées en 2019')\n",
    "            st.write(\"On observe de nombreuse données manquantes, mal codées ou nécessitant un pré-traitement. Nous allons d'abords nettoyer les données en supprimans les colonnes où le pourcentage de données manquantes est trop importantes. Ensuite, nous supprimerons les lignes où des données manquent encore à l'appel. On intégrera de nouvelles données et on transformera les colonnes dont le type n'est pas correct. On inserera de nouvelles colonne et on remplira celle dont les valeurs manquantes peuvent être déduites.\")\n",
    "            st.caption('Dataframe initial')\n",
    "            st.dataframe(df.head())\n",
    "\n",
    "    elif (option_data == 'Valeurs Foncières 2020'):\n",
    "            df = df_2020\n",
    "            st.markdown('##### Données des valeurs foncières déclarées en 2020')\n",
    "            st.write(\"On observe de nombreuse données manquantes, mal codées ou nécessitant un pré-traitement. Nous allons d'abords nettoyer les données en supprimans les colonnes où le pourcentage de données manquantes est trop importantes. Ensuite, nous supprimerons les lignes où des données manquent encore à l'appel. On intégrera de nouvelles données et on transformera les colonnes dont le type n'est pas correct. On inserera de nouvelles colonne et on remplira celle dont les valeurs manquantes peuvent être déduites.\")\n",
    "            st.caption('Dataframe initial')\n",
    "            st.dataframe(df.head()) \n",
    "\n",
    "    st.markdown(\"**Pour visualiser le traitement des données, veuillez cliquer sur le bouton `Voir le traitement des données`.**\")\n",
    "    \n",
    "    col1, col2, col3 = st.columns(3)\n",
    "\n",
    "    with col1:\n",
    "            pass\n",
    "    with col3:\n",
    "            pass\n",
    "    with col2 :\n",
    "            btn = st.button('Voir le traitement des données')\n",
    "\n",
    "    if btn:\n",
    "            st.subheader('Traitement des données')\n",
    "            \n",
    "            miss = missing_rate(df)\n",
    "            miss.name = 'missing_rate'\n",
    "            st.markdown('##### Nettoyage des données')\n",
    "            st.caption(\"Rapport de données manquantes par variables\")\n",
    "            col1, col2 = st.columns([3,1])\n",
    "            col1.dataframe(miss)\n",
    "            col2.markdown(\"<p style='text-align: justify; color: grey; position:left;'>Nous allons supprimer toute les colonnes où le taux de valeurs manquantes est supérieurs à 50%, afin de ne remplacer qu'un minimum de valeurs manquantes.\",unsafe_allow_html=True)\n",
    "            \n",
    "            df = cleaning(df, miss.index)\n",
    "            st.caption('Dataframe nettoyé')   \n",
    "            st.dataframe(df.head()) \n",
    "            st.markdown(\"On s'assure que le dataframe ne contient plus de valeure manquante :\")\n",
    "            st.code('df.isnull().values.any() == False')\n",
    "\n",
    "            if (df.isnull().values.any() == False):\n",
    "                    st.code(\">>> True\")\n",
    "\n",
    "            df = pre_processing(df)\n",
    "            st.markdown('##### Transformation des données')\n",
    "            st.caption(\"Modification des types\")\n",
    "            st.image('./img/img1.jpg')\n",
    "            st.markdown(\"On corrige l'encodage des données puis nous allons integrer de nouvelles variables afin d'enrichir notre dataframe et d'effectuer différent traitements dessus.\")\n",
    "\n",
    "            df = processing(df)\n",
    "            group_by_commune = process_commune(df)\n",
    "            group_by_departement = process_departement(df)\n",
    "            group_by_commune_square = process_commune_square(df)\n",
    "            group_by_departement_square = process_departement_square(df)\n",
    "            \n",
    "            st.markdown('##### Intégration des données')\n",
    "            st.caption('Dataframe enrichie')\n",
    "            st.dataframe(df.head())\n",
    "            \n",
    "            col1, col2 = st.columns(2)\n",
    "            col1.caption('Données par commune')\n",
    "            col2.caption('Prix moyen par commune')\n",
    "\n",
    "            col1, col2 = st.columns(2)\n",
    "            group_by_commune_tail = group_by_commune.tail()\n",
    "            group_by_commune_square_tail = group_by_commune_square.tail()\n",
    "            col1.table(group_by_commune_tail.to_frame().style.highlight_max(axis=0))\n",
    "            col2.table(group_by_commune_square_tail.style.highlight_max(axis=0))\n",
    "\n",
    "            col1, col2 = st.columns(2)\n",
    "            col1.caption('Données par département')\n",
    "            col2.caption('Prix moyen par département')\n",
    "\n",
    "            col1, col2 = st.columns(2)\n",
    "            group_by_departement_tail = group_by_departement.tail()\n",
    "            group_by_departement_square_tail = group_by_departement_square.tail()\n",
    "            col1.table(group_by_departement_tail.to_frame().style.highlight_max(axis=0))\n",
    "            col2.table(group_by_departement_square_tail.style.highlight_max(axis=0))\n",
    "            \n",
    "            agree = st.checkbox('Voir moins')\n",
    "\n",
    "            if agree:\n",
    "                    btn = False\n",
    "\n",
    "    else:\n",
    "            miss = missing_rate(df)\n",
    "            df = cleaning(df, miss.index)\n",
    "            df = pre_processing(df)\n",
    "            df = processing(df)\n",
    "           \n",
    "    st.caption('Dataframe final')\n",
    "    st.dataframe(df.head())\n",
    "    st.write(\"On peut désormais effectué différents traitements sur le dataframe car il est correctement implémenté. Nous allons dans un premier temps analyser les données dans leurs ensembles et s'intéresser au information comme le taux de croissance, les villes/départements/régions avec le plus de transactions immobilière/le plus prix moyen au mètre carré. Nous visuliserons également la fréquence de transaction selon les mois et selon le type de local.\")\n",
    "    st.write(\"Dans un autre temps, nous gererons la visualisation des données selon les différents traitements effectués précédement comme par exemple le prix aumètre carré moyen par région ou selon différents mois.\")\n",
    "    st.markdown(\"**Pour visualiser les différents traitements, utiliser les `widgets` dans la slidebar.**\")\n",
    "    \n",
    "    return df, df_2016, df_2017, df_2018, df_2019, df_2020           \n",
    "\n",
    "df, df_2016, df_2017, df_2018, df_2019, df_2020 = init_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisation de la sidebar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/projet_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/projet_app.py\n",
    "\n",
    "# Création des fonctionnalités de la sidebar:\n",
    "@log\n",
    "def init_sidebar(df):\n",
    "        \n",
    "        pick = None\n",
    "        options = None\n",
    "        format = 'Y-M'  # format output\n",
    "        start_date = dt.date(2016, 1, 1)\n",
    "        end_date = dt.date(2016, 12, 30)\n",
    "\n",
    "        if (option_data == 'Valeurs Foncières 2016'):\n",
    "                start_date = dt.date(2016, 1, 1)\n",
    "                end_date = dt.date(2016, 12, 30)\n",
    "\n",
    "        elif (option_data == 'Valeurs Foncières 2017'):\n",
    "                start_date = dt.date(2017, 1, 1)\n",
    "                end_date = dt.date(2017, 12, 30)\n",
    "\n",
    "        elif (option_data == 'Valeurs Foncières 2018'):\n",
    "                start_date = dt.date(2018, 1, 1)\n",
    "                end_date = dt.date(2018, 12, 30)\n",
    "\n",
    "        elif (option_data == 'Valeurs Foncières 2019'):\n",
    "                start_date = dt.date(2019, 1, 1)\n",
    "                end_date = dt.date(2019, 12, 30)\n",
    "\n",
    "        elif (option_data == 'Valeurs Foncières 2020'):\n",
    "                start_date = dt.date(2020, 1, 1)\n",
    "                end_date = dt.date(2020, 12, 30)\n",
    "\n",
    "        agree = st.sidebar.checkbox('Spécifier un date')\n",
    "        if agree:\n",
    "                pick = st.sidebar.slider('Choisir un mois spécifique', min_value=start_date, value=end_date ,max_value=end_date, format=format)\n",
    "                st.sidebar.write(\"Mois choisie :\", pick.strftime('%b'))\n",
    "                mask1 = df['mth'] == pick.month\n",
    "                df = df[mask1]\n",
    "  \n",
    "        else:\n",
    "                pick = 1;\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "        options = st.sidebar.multiselect('Quelles régions ?',\n",
    "        [\"AUVERGNE-RHÔNE-ALPES\", \"BOURGOGNE-FRANCHE-COMTÉ\", \"BRETAGNE\", \"CENTRE-VAL-DE-LOIRE\", \"CORSE\",\n",
    "        \"ÎLE-DE-FRANCE\", \"GRAND-EST\",\"HAUT-DE-FRANCE\",\"NORMANDIE\", \"NOUVELLE-AQUITAINE\", \"OCCITANIE\", \"PAYS-DE-LA-LOIRE\", \"PROVENCE-ALPES-CÔTE D'AZUR\"])\n",
    "        \n",
    "        if options:\n",
    "                mask2 = df['region'].isin(options)\n",
    "                df = df[mask2]\n",
    "                \n",
    "        else:\n",
    "                options = 1;\n",
    "       \n",
    "        st.caption('Dataframe réduit')\n",
    "        st.dataframe(df.head())\n",
    "        st.text(\"La taille du dataframe séléctionné est :\" + str(df.shape))\n",
    "\n",
    "        return pick, options\n",
    "pick, options = init_sidebar(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/projet_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/projet_app.py\n",
    "\n",
    "@log\n",
    "def init_charts(df):\n",
    "\n",
    "    st.subheader('Visualisation des données sélectionnées')\n",
    "    st.markdown(\"<p style='text-align: justify; color: grey; position:left;'>Nous allons visualiser les données à travers différents graphiques et traitements. Nous analyseron la fréquences de transactions financière, le taux de croissance, le prix au mètre carré moyen selon plusieurs paramètres et catégories comme le type de local, la région, le mois, ... \",unsafe_allow_html=True)\n",
    "\n",
    "    if (type(pick) != int):\n",
    "        mask1 = df['mth'] == pick.month\n",
    "        df = df[mask1]\n",
    "    \n",
    "    if (type(options) != int):\n",
    "        mask2 = df['region'].isin(options)\n",
    "        df = df[mask2]\n",
    "\n",
    "    st.markdown(\"##### Fréquence des transaction finnancière\")\n",
    "    st.write(\"Nous allons nous interesser ici aux nombre de transactions finnancières. On peut visualiser la fréquences selon les communes, les départements, les régions et les mois. Vous pouvez également y appliquer les différents paramètres de la sidebar.\")\n",
    "    \n",
    "    group_by_commune = process_commune(df)\n",
    "    group_by_commune.name = 'Transactions'\n",
    "    group_by_departement = process_departement(df)\n",
    "    group_by_departement.name = 'Transactions'\n",
    "    group_by_commune_square = process_commune_square(df)\n",
    "    group_by_commune_square.name = 'Prix moyen'\n",
    "\n",
    "    col1, col2 = st.columns(2)\n",
    "    col1.metric(\"Ville avec le plus de transactions :\",group_by_commune.idxmax(), group_by_commune.tail(1).item())\n",
    "    col2.metric(\"Département avec le plus de transactions :\", group_by_departement.idxmax(), group_by_departement.tail(1).item())\n",
    "\n",
    "    st.caption(\"Nombre de transactions par commune - Top 10\")\n",
    "    st.area_chart(group_by_commune.tail(10))\n",
    "\n",
    "\n",
    "    st.caption(\"Nombre de transactions par département - Top 10\")\n",
    "    st.area_chart(group_by_departement.tail(10))\n",
    "\n",
    "    fig1 = plt.figure()\n",
    "    x1 = group_by_commune.tail(10)\n",
    "    ax = x1.plot(kind=\"bar\", label = \"Top 10 communes\", width=0.5)\n",
    "    ax.set_xlabel('Communes')\n",
    "    ax.set_ylabel('Fréquence transaction')\n",
    "    ax.set_title('Fréquence par commune - Top 10')\n",
    "\n",
    "    fig2 = plt.figure()\n",
    "    x2 = group_by_departement.tail(10)\n",
    "    ax = x2.plot(kind=\"bar\", label = \"Top 10 départements\", width=0.5)\n",
    "    ax.set_xlabel('Département')\n",
    "    ax.set_ylabel('Fréquence transaction')\n",
    "    ax.set_title('Fréquence par département - Top 10')\n",
    "\n",
    "    col1, col2 = st.columns([3,2])\n",
    "    col1.caption('Histogramme trié des fréquences par commune')\n",
    "    col2.caption('Dataframe des fréquences par commune')\n",
    "\n",
    "    col1, col2 = st.columns([3,2])\n",
    "    col1.pyplot(fig1)\n",
    "    col2.write(group_by_commune.tail(10))\n",
    "    \n",
    "    col1, col2 = st.columns([2,3])\n",
    "    col2.caption('Histogramme trié des fréquences par département')\n",
    "    col1.caption('Dataframe des fréquences par département')\n",
    "\n",
    "    col1, col2 = st.columns([2,3])\n",
    "    col2.pyplot(fig2)\n",
    "    col1.write(group_by_departement.tail(10))\n",
    "    \n",
    "    plt.set_loglevel('WARNING')\n",
    "\n",
    "        \n",
    "    if not(type(pick) != int):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.hist(df.mth, bins=12, rwidth=0.8, label = \"Mois\")\n",
    "        ax.set_title(\"Fréquence par mois\")\n",
    "        ax.set_xlabel(\"Mois\")\n",
    "        ax.set_ylabel(\"Nombre de transactions\")\n",
    "        ax.set_xticks(range(1,13))\n",
    "        ax.set_xticklabels(['JANV','FEV','MARS','AVRL','BMAI','JUIN','JUILLT', 'AOUT', 'SEPT', 'OCT', 'NOV', 'DEC'])\n",
    "        st.caption(\"Histogramme des fréquences par mois\")\n",
    "        st.pyplot(fig)\n",
    "\n",
    "    else:\n",
    "\n",
    "        group_by_dom = process_month(df)\n",
    "        st.caption(\"Fréquences par jours du mois\")\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(group_by_dom.index, group_by_dom)\n",
    "        ax.set_title(\"Fréquence par jour du mois\")\n",
    "        ax.set_xlabel(\"Jours du mois\")\n",
    "        ax.set_ylabel(\"Nombre de transactions\")\n",
    "        st.pyplot(fig)\n",
    "\n",
    "    \n",
    "    if not(type(options) != int):\n",
    "\n",
    "        labels = [\"AVRGN\", \"BRGN\", \n",
    "                    \"BRTN\", \"VDL\", \"CRS\",\n",
    "                    \"IDF\", \"GRD-EST\",\"HDF\",\n",
    "                    \"NRMND\", \"NVL-ACQT\", \"OCTN\", \n",
    "                    \"PDLL\", \"PACA\"]\n",
    "        st.caption('Histogramme des fréquences par région')\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.hist(df['region'], bins=13, rwidth=0.8)\n",
    "        ax.set_xlabel('Régions')\n",
    "        ax.xaxis.set_tick_params(labelsize=6)\n",
    "        ax.set_xticks(range(0,13))\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.set_ylabel('Nombre de transactions')\n",
    "        ax.set_title('Fréquence par région')\n",
    "\n",
    "        st.pyplot(fig)\n",
    "\n",
    "        group_by_local_region = process_local_region(df)\n",
    "        group_by_local_region.columns = group_by_local_region.columns.values\n",
    "        st.caption('Fréquences des transactions par région selon le type de local')\n",
    "        st.bar_chart(group_by_local_region)\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        group_by_departement_region = process_region(df)\n",
    "        st.caption('Nuage de points des fréquences par département selon chaque région')\n",
    "        \n",
    "        fig,ax = plt.subplots()\n",
    "        plt.set_loglevel('WARNING')\n",
    "        ax.plot(group_by_departement_region,'o', label=group_by_departement_region.columns.values)\n",
    "        ax.legend(loc='upper right', prop={'size': 6})\n",
    "        ax.xaxis.set_tick_params(labelsize=4) \n",
    "        ax.set_xlabel('Département')\n",
    "        ax.set_ylabel('Fréquence')\n",
    "        ax.set_title('Fréquence par département selon chaque région')\n",
    "\n",
    "        st.pyplot(fig)\n",
    "\n",
    "        if (len(options) != 1):\n",
    "            group_by_local_region = process_local_region(df)\n",
    "            group_by_local_region.columns = group_by_local_region.columns.values\n",
    "            st.caption('Fréquences des transactions par région selon le type de local')\n",
    "            st.bar_chart(group_by_local_region)\n",
    "        else :\n",
    "            group_by_local = process_local(df)\n",
    "            group_by_local.columns = group_by_local.columns.values\n",
    "            st.caption('Fréquences des transactions par région selon le type de local')\n",
    "            st.line_chart(group_by_local)\n",
    "        \n",
    "\n",
    "init_charts(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation des traitements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/projet_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/projet_app.py\n",
    "\n",
    "@log\n",
    "def init_square(df):\n",
    "\n",
    "    st.markdown('##### Visualisation des prix des biens fonciers')\n",
    "    st.write(\"On s'intéresse ici au prix du terrain (en mètre carré). Nous avons divisé la valeurf foncière par la surface du terrain, obtenant ainsi cette variable. Grace à différents graphique (ligne, nuage de point, histogramme) nous allons pouvoir en tirer quelque analyses.\")\n",
    "\n",
    "    if (type(pick) != int):\n",
    "        mask1 = df['mth'] == pick.month\n",
    "        df = df[mask1]\n",
    "    \n",
    "    if (type(options) != int):\n",
    "        mask2 = df['region'].isin(options)\n",
    "        df = df[mask2]  \n",
    "        \n",
    "    group_by_commune_square = process_commune_square(df)\n",
    "    group_by_commune_square.name = 'Prix moyen'\n",
    "  \n",
    "    group_by_departement_square = process_departement_square(df)\n",
    "    group_by_departement_square.name = 'Prix moyen'\n",
    "\n",
    "    col1, col2 = st.columns(2)\n",
    "    col1.metric(\"Ville avec le prix moyen le plus haut:\",group_by_commune_square.tail(1).last_valid_index(), str(group_by_commune_square.values[-1:]).strip('[]') + \"€\")\n",
    "    col2.metric(\"Département avec le prix moyen le plus haut:\", group_by_departement_square.tail(1).last_valid_index(), str(group_by_departement_square.values[-1:]).strip('[]') + \"€\")\n",
    "\n",
    "    st.caption(\"Prix moyen par commune - Top 10\")\n",
    "    st.bar_chart(group_by_commune_square.tail(10))\n",
    "\n",
    "    \n",
    "    st.caption(\"Prix moyen par departement - Top 10\")\n",
    "    st.bar_chart(group_by_departement_square.tail(10))\n",
    "    \n",
    "    group_by_square = process_square(df)\n",
    "    group_by_square  = group_by_square.reset_index()\n",
    "\n",
    "\n",
    "    # Mask pour s'assurer d'avoir une visualisation correcte : on n'affiche pas les données dont l'achat a couté plus de 1 millions d'euros ou dont la surface dépasse 2000 mètres carrés\n",
    "    if (type(options) != int):\n",
    "\n",
    "        c = alt.Chart(group_by_square).mark_circle().encode(\n",
    "            x='valeur_fonciere', y='surface_terrain', size='region', color='region', tooltip=['code_departement','region','valeur_fonciere','surface_terrain'])\n",
    "\n",
    "        st.caption('Rapport moyen (Valeurs_Foncière/Surface) dans les différents départements de chaque région')\n",
    "        st.altair_chart(c, use_container_width=True)\n",
    "\n",
    "        group_by_commune_departement_square = process_commune_departement_square(df)\n",
    "\n",
    "        group_by_commune_departement_square = group_by_commune_departement_square.reset_index()\n",
    "\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(20,5))\n",
    "        ax.scatter(group_by_commune_departement_square['code_departement'], group_by_commune_departement_square['price_by_square'])\n",
    "\n",
    "        ax.set_xlabel('Régions')\n",
    "        ax.set_ylabel('Prix moyen')\n",
    "        ax.set_ylim(0,200000)\n",
    "        ax.xaxis.set_tick_params(labelsize=8)\n",
    "        ax.set_title('Prix moyen pour chaque ville de chaque region')\n",
    "        st.caption(\"Prix moyen des villes de chaque region\")\n",
    "        st.pyplot(fig)\n",
    "    \n",
    "        st.caption('Prix moyen des transactions pour chaque departement de chaque région')\n",
    "        c = alt.Chart(df).mark_circle().encode(x='code_departement', y='price_by_square', size='region', color='region', tooltip=['nom_commune','region','code_departement','price_by_square'])\n",
    "\n",
    "        st.altair_chart(c, use_container_width=True)\n",
    "        \n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        mask = group_by_square['valeur_fonciere'] < 1000000\n",
    "        group_by_square = group_by_square[mask]\n",
    "        mask = group_by_square['surface_terrain'] < 2000\n",
    "        group_by_square = group_by_square[mask]\n",
    "\n",
    "\n",
    "        c = alt.Chart(group_by_square).mark_circle().encode(\n",
    "            x='valeur_fonciere', y='surface_terrain', size='region', color='region', tooltip=['code_departement','region','valeur_fonciere','surface_terrain'])\n",
    "\n",
    "        st.caption('Rapport moyen Valeurs dans les différents départements de chaque région (sans IDF)')\n",
    "        st.altair_chart(c, use_container_width=True)\n",
    "\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(20,5))\n",
    "        ax.scatter(df['region'], df['price_by_square'])\n",
    "\n",
    "        ax.set_xlabel('Départements')\n",
    "        ax.set_ylabel('Prix moyen')\n",
    "    \n",
    "        ax.xaxis.set_tick_params(labelsize=8)\n",
    "        ax.set_title('Prix moyen pour chaque transactions de chaque département')\n",
    "\n",
    "        st.caption('Prix moyen des transactions pour chaque département')\n",
    "        st.pyplot(fig)\n",
    "\n",
    "\n",
    "init_square(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map et Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/projet_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/projet_app.py\n",
    "\n",
    "@log\n",
    "def init_map(df):\n",
    "\n",
    "    st.markdown('##### Visualisation des positions etde la map')\n",
    "    st.write(\"On s'intéresse désormais à la position des individus, à leurs densités, leurs représentation sur une map, ... Nous allons visualiser ces éléments afin d'en dégager plusieurs analyses.\")\n",
    "\n",
    "    if (type(pick) != int):\n",
    "        mask1 = df['mth'] == pick.month\n",
    "        df = df[mask1]\n",
    "    \n",
    "    if (type(options) != int):\n",
    "        mask2 = df['region'].isin(options)\n",
    "        df = df[mask2]  \n",
    "\n",
    "    st.caption('Nuage de points des positions')\n",
    "    fig, axs = plt.subplots()\n",
    "    axs.plot(df.longitude,df.latitude,'.', alpha=.5)\n",
    "    axs.set_title(\"Positions\")\n",
    "    axs.set_xlabel(\"Longitude\")\n",
    "    axs.set_ylabel(\"Latitude\")\n",
    "    st.pyplot(fig)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(df.latitude,alpha=0.5, color='b', label = \"Latitude\")\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.twiny()\n",
    "    ax.hist(df.longitude,alpha=0.5, color='r', label = \"Longitude\")\n",
    "    ax.set_title(\"Histogramme Latitude/Longitude\")\n",
    "    ax.legend(loc='upper left')\n",
    "    st.pyplot(fig)\n",
    "\n",
    "    df = df.sample(frac =.04)\n",
    "    st.pydeck_chart(pdk.Deck(\n",
    "        map_style='mapbox://styles/mapbox/light-v9',\n",
    "        initial_view_state=pdk.ViewState(\n",
    "        latitude=48.8534,\n",
    "        longitude=2.3488,\n",
    "        zoom=12,\n",
    "        pitch=50,\n",
    "        ),\n",
    "        layers=[\n",
    "        pdk.Layer('HexagonLayer',\n",
    "        data=df,\n",
    "        get_position='[longitude, latitude]',\n",
    "        radius=200,\n",
    "        elevation_scale=4,\n",
    "        elevation_range=[0, 1000],\n",
    "        pickable=True,\n",
    "        extruded=True,),\n",
    "        pdk.Layer(\n",
    "        'ScatterplotLayer',\n",
    "        data=df,\n",
    "        get_position='[longitude, latitude]',\n",
    "        get_radius=200,\n",
    "        ),\n",
    "        ],))\n",
    "        \n",
    "init_map(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://192.168.0.152:8501\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  For better performance, install the Watchdog module:\u001b[0m\n",
      "\n",
      "  $ xcode-select --install\n",
      "  $ pip install watchdog\n",
      "            \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!streamlit run src/projet_app.py"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9e7d9e18a3f8d21d003bad8284ff2d39c766bc68d152ddf3ad9811e2e957dfc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('venvdtv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
